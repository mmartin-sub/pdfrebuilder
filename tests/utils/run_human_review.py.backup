#!/usr/bin/env python3
"""
Human Review Test Runner

This script runs the human review tests and provides clear instructions
for manual validation of the generated outputs.

Usage:
------
python tests/run_human_review.py [options]

Options:
  --test-type TEXT_ONLY|DRAWINGS_ONLY|MIXED|ALL  Type of tests to run
  --keep-outputs                                  Keep all generated files
  --open-outputs                                  Attempt to open PDFs for review
"""

import argparse
import logging
import sys
from pathlib import Path

# Import secure subprocess alternatives
try:
    from src.security.subprocess_compatibility import run
except ImportError:
    # Fallback to standard subprocess if secure modules not available
    import subprocess

    run = subprocess.run

# Suppress fontTools debug logging
logging.getLogger("fontTools").setLevel(logging.WARNING)
logging.getLogger("fontTools.ttLib").setLevel(logging.WARNING)
logging.getLogger("fontTools.ttLib.ttFont").setLevel(logging.WARNING)


def run_human_review_tests(test_type="ALL", keep_outputs=True, open_outputs=False):
    """Run human review tests with specified options"""

    print("=" * 80)
    print("PDF PROCESSING PIPELINE - HUMAN REVIEW TEST RUNNER")
    print("=" * 80)

    # Ensure output directory exists
    review_dir = Path("tests/human_review_outputs")
    review_dir.mkdir(exist_ok=True)

    # Build pytest command
    cmd = [
        "hatch",
        "run",
        "test",
        "-m",
        "human_review",
        "-v",
        "-s",  # verbose and no capture for clear output
        "--disable-warnings",  # suppress pytest warnings
        "tests/test_e2e_human_review.py",
    ]

    # Add test type filter if specified
    if test_type != "ALL":
        if test_type == "TEXT_ONLY":
            cmd.extend(["-k", "text_elements"])
        elif test_type == "DRAWINGS_ONLY":
            cmd.extend(["-k", "drawing_elements"])
        elif test_type == "MIXED":
            cmd.extend(["-k", "full_pipeline"])

    print(f"Running command: {' '.join(cmd)}")
    print(f"Output directory: {review_dir.absolute()}")
    print()

    # Run the tests
    try:
        result = run(cmd, check=False)

        if result.returncode == 0:
            print("\n" + "=" * 80)
            print("‚úÖ TESTS COMPLETED SUCCESSFULLY")
            print("=" * 80)
        else:
            print("\n" + "=" * 80)
            print("‚ö†Ô∏è  TESTS COMPLETED WITH ISSUES")
            print("=" * 80)

    except KeyboardInterrupt:
        print("\n" + "=" * 80)
        print("‚ùå TESTS INTERRUPTED BY USER")
        print("=" * 80)
        return 1
    except Exception as e:
        print(f"\n‚ùå ERROR RUNNING TESTS: {e}")
        return 1

    # List generated files
    print("\nGENERATED FILES FOR REVIEW:")
    print("-" * 40)

    output_files = list(review_dir.glob("*.pdf")) + list(review_dir.glob("*.json"))
    output_files.sort()

    if not output_files:
        print("No output files found. Check test execution.")
        return 1

    # Group files by type
    input_files = [f for f in output_files if "input" in f.name]
    output_pdfs = [f for f in output_files if "output" in f.name and f.suffix == ".pdf"]
    config_files = [f for f in output_files if f.suffix == ".json"]
    debug_files = [f for f in output_files if "debug" in f.name]

    for category, files in [
        ("INPUT PDFs", input_files),
        ("OUTPUT PDFs", output_pdfs),
        ("CONFIG FILES", config_files),
        ("DEBUG PDFs", debug_files),
    ]:
        if files:
            print(f"\n{category}:")
            for file in files:
                size_kb = file.stat().st_size // 1024
                print(f"  üìÑ {file.name} ({size_kb} KB)")

    # Provide review instructions
    print("\n" + "=" * 80)
    print("MANUAL REVIEW INSTRUCTIONS")
    print("=" * 80)
    print("1. Compare INPUT and OUTPUT PDFs side-by-side")
    print("2. Check that all content is preserved accurately")
    print("3. Verify text rendering, fonts, and positioning")
    print("4. Validate drawing elements, colors, and shapes")
    print("5. Review DEBUG PDFs for layer-by-layer analysis")
    print("6. Check CONFIG JSON files for data completeness")

    print(f"\nAll files are located in: {review_dir.absolute()}")

    # Optionally open files for review
    if open_outputs and output_pdfs:
        print("\nAttempting to open PDFs for review...")
        for pdf_file in output_pdfs[:3]:  # Limit to first 3 to avoid overwhelming
            try:
                if sys.platform == "darwin":  # macOS
                    run(["open", str(pdf_file)], check=False)
                elif sys.platform == "linux":  # Linux
                    run(["xdg-open", str(pdf_file)], check=False)
                elif sys.platform == "win32":  # Windows
                    # Use os.startfile instead of shell command for security
                    import os

                    os.startfile(str(pdf_file))
                print(f"  üìñ Opened: {pdf_file.name}")
            except Exception as e:
                print(f"  ‚ùå Could not open {pdf_file.name}: {e}")

    # Cleanup option
    if not keep_outputs:
        print(f"\nCleaning up outputs in {review_dir}...")
        for file in output_files:
            try:
                file.unlink()
                print(f"  üóëÔ∏è  Deleted: {file.name}")
            except Exception as e:
                print(f"  ‚ùå Could not delete {file.name}: {e}")

    print("\n" + "=" * 80)
    print("REVIEW COMPLETE")
    print("=" * 80)

    return result.returncode


def main():
    """Main entry point"""
    parser = argparse.ArgumentParser(
        description="Run human review tests for PDF processing pipeline",
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )

    parser.add_argument(
        "--test-type",
        choices=["TEXT_ONLY", "DRAWINGS_ONLY", "MIXED", "ALL"],
        default="ALL",
        help="Type of tests to run (default: ALL)",
    )

    parser.add_argument(
        "--keep-outputs",
        action="store_true",
        default=True,
        help="Keep all generated files (default: True)",
    )

    parser.add_argument(
        "--clean-outputs",
        action="store_true",
        help="Clean up generated files after review",
    )

    parser.add_argument(
        "--open-outputs",
        action="store_true",
        help="Attempt to open generated PDFs for review",
    )

    args = parser.parse_args()

    # Handle conflicting options
    keep_outputs = args.keep_outputs and not args.clean_outputs

    return run_human_review_tests(
        test_type=args.test_type,
        keep_outputs=keep_outputs,
        open_outputs=args.open_outputs,
    )


if __name__ == "__main__":
    sys.exit(main())
