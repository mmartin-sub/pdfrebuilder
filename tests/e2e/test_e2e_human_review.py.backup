"""
End-to-End PDF Processing Test with Human Review

This test leverages the existing e2e test infrastructure but generates outputs
specifically for human review and comparison. It reuses the same pipeline
functions and test utilities while providing clear instructions for manual
validation.

Usage:
------
Run with pytest markers for human review tests:

`hatch run test -m human_review -v tests/test_e2e_human_review.py`

Or run specific human review tests:

`hatch run test -k "human_review" -v tests/test_e2e_human_review.py`

Generated Files for Human Review:
---------------------------------
All files are generated in the test output directory with clear naming:

1. **Input Files**: Original PDFs used as test sources
2. **Output Files**: Generated PDFs from the pipeline
3. **Config Files**: JSON configurations extracted from PDFs
4. **Debug Files**: Layer-by-layer visualization PDFs
5. **Comparison Reports**: Side-by-side comparison data

The test will print clear paths to all generated files for easy access.
"""

import argparse
import json
import logging
import sys
from io import StringIO
from pathlib import Path

import pytest
from main import run_pipeline

from src.engine.visual_validator import validate_documents
from src.generate_debug_pdf_layers import generate_debug_pdf_layers
from tests.config import get_unique_id
from tests.test_e2e_pdf_pipeline import create_comprehensive_drawing_test_config
from tests.test_e2e_pdf_regeneration import create_pdf_with_elements

# Suppress fontTools debug logging immediately
logging.getLogger("fontTools").setLevel(logging.WARNING)
logging.getLogger("fontTools.ttLib").setLevel(logging.WARNING)
logging.getLogger("fontTools.ttLib.ttFont").setLevel(logging.WARNING)


@pytest.fixture(scope="module")
def human_review_output_dir():
    """Create a persistent directory for human review outputs"""
    review_dir = Path("tests/human_review_outputs")
    review_dir.mkdir(exist_ok=True)
    return review_dir


@pytest.fixture
def review_unique_id():
    """Generate a unique ID for this review session"""
    return get_unique_id()


def print_review_header(test_name: str):
    """Print a clear header for human review instructions"""
    print(f"\n{'=' * 80}")
    print(f"HUMAN REVIEW REQUIRED: {test_name}")
    print(f"{'=' * 80}")


def print_review_footer():
    """Print footer for human review section"""
    print(f"{'=' * 80}\n")


def run_pipeline_with_capture(args):
    """Run the pipeline and capture output, checking for errors"""
    old_stdout = sys.stdout
    captured_output = StringIO()
    sys.stdout = captured_output

    try:
        run_pipeline(args)
        output = captured_output.getvalue()
        if "❌" in output:
            raise AssertionError(f"Pipeline failed:\n{output}")
        return output
    finally:
        sys.stdout = old_stdout


@pytest.mark.human_review
@pytest.mark.e2e
@pytest.mark.skip(reason="Human review test - run manually with -m human_review")
def test_text_elements_human_review(human_review_output_dir, review_unique_id):
    """Generate text-focused PDFs for human review of text extraction/regeneration"""

    print_review_header("Text Elements Processing")

    # Create test elements focused on text
    text_elements = [
        {
            "type": "text",
            "id": f"text_{i}",
            "content": f"Sample Text {i} - Font Variation Test",
        }
        for i in range(1, 6)
    ]

    # Generate unique file paths
    unique = review_unique_id
    input_pdf = human_review_output_dir / f"text_input_{unique}.pdf"
    output_pdf = human_review_output_dir / f"text_output_{unique}.pdf"
    config_json = human_review_output_dir / f"text_config_{unique}.json"
    debug_pdf = human_review_output_dir / f"text_debug_{unique}.pdf"

    # Step 1: Create input PDF with text elements
    create_pdf_with_elements(text_elements, str(input_pdf))

    # Step 2: Extract configuration
    args_extract = argparse.Namespace(
        log_level="DEBUG",
        mode="extract",
        input=str(input_pdf),
        output=None,
        config_file=None,  # Don't load PDFRebuilderConfig in generate mode
        config=str(config_json),  # Document structure JSON file
        debugoutput=None,
        extract_text=True,
        extract_images=True,
        extract_drawings=True,
        extract_raw_backgrounds=False,
        output_dir="./output",
        test_output_dir=str(human_review_output_dir),
        reports_output_dir="./output/reports",
        temp_dir=None,
        input_engine="fitz",  # Add missing input_engine
        log_file=None,  # Add missing log_file
    )

    run_pipeline_with_capture(args_extract)

    # Step 3: Generate output PDF
    args_generate = argparse.Namespace(
        log_level="DEBUG",
        mode="generate",
        input=None,
        output=str(output_pdf),
        config_file=str(config_json),
        debugoutput=None,
        extract_text=True,
        extract_images=True,
        extract_drawings=True,
        extract_raw_backgrounds=False,
        output_dir="./output",
        test_output_dir=str(human_review_output_dir),
        reports_output_dir="./output/reports",
        temp_dir=None,
    )

    run_pipeline_with_capture(args_generate)

    # Step 4: Generate debug PDF
    if config_json.exists():
        generate_debug_pdf_layers(str(config_json), str(debug_pdf))

    # Step 5: Generate comparison report
    comparison_result = None
    if input_pdf.exists() and output_pdf.exists():
        try:
            comparison_result = validate_documents(str(input_pdf), str(output_pdf))
        except Exception as e:
            print(f"Warning: Could not generate comparison: {e}")

    # Print review instructions
    print("TEXT PROCESSING REVIEW:")
    print(f"  Input PDF:  {input_pdf}")
    print(f"  Output PDF: {output_pdf}")
    print(f"  Config:     {config_json}")
    print(f"  Debug PDF:  {debug_pdf}")

    if comparison_result:
        print(f"  SSIM Score: {comparison_result.ssim_score:.4f}")
        if comparison_result.ssim_score < 0.95:
            print("  ⚠️  Low similarity score - check for text rendering issues")

    print("\nREVIEW CHECKLIST:")
    print("  □ Text content matches between input and output")
    print("  □ Font rendering is consistent")
    print("  □ Text positioning is accurate")
    print("  □ No missing or corrupted characters")

    # Basic assertions
    assert input_pdf.exists() and input_pdf.stat().st_size > 0
    assert output_pdf.exists() and output_pdf.stat().st_size > 0
    assert config_json.exists() and config_json.stat().st_size > 0

    print_review_footer()


@pytest.mark.human_review
@pytest.mark.e2e
@pytest.mark.skip(reason="Human review test - run manually with -m human_review")
def test_drawing_elements_human_review(human_review_output_dir, review_unique_id):
    """Generate drawing-focused PDFs for human review of vector graphics processing"""

    print_review_header("Drawing Elements Processing")

    # Use the comprehensive drawing test config from existing e2e tests
    test_config = create_comprehensive_drawing_test_config()

    # Generate unique file paths
    unique = review_unique_id
    config_json = human_review_output_dir / f"drawing_config_{unique}.json"
    output_pdf = human_review_output_dir / f"drawing_output_{unique}.pdf"
    debug_pdf = human_review_output_dir / f"drawing_debug_{unique}.pdf"

    # Step 1: Save test configuration
    with open(config_json, "w") as f:
        json.dump(test_config, f, indent=2)

    # Step 2: Generate PDF from configuration
    args_generate = argparse.Namespace(
        log_level="DEBUG",
        mode="generate",
        input=None,
        output=str(output_pdf),
        config_file=str(config_json),
        debugoutput=None,
        extract_text=True,
        extract_images=True,
        extract_drawings=True,
        extract_raw_backgrounds=False,
        output_dir="./output",
        test_output_dir=str(human_review_output_dir),
        reports_output_dir="./output/reports",
        temp_dir=None,
    )

    run_pipeline_with_capture(args_generate)

    # Step 3: Generate debug PDF
    generate_debug_pdf_layers(str(config_json), str(debug_pdf))

    # Print review instructions
    print("DRAWING PROCESSING REVIEW:")
    print(f"  Config:     {config_json}")
    print(f"  Output PDF: {output_pdf}")
    print(f"  Debug PDF:  {debug_pdf}")

    print("\nEXPECTED DRAWING ELEMENTS:")
    print("  □ Rectangle with stroke and fill")
    print("  □ Rectangle with stroke only (red)")
    print("  □ Ellipse with fill only (blue)")
    print("  □ Complex path with bezier curves")
    print("  □ Simple lines (cross pattern)")
    print("  □ Triangle with orange fill")
    print("  □ Rounded square with light blue fill")
    print("  □ Multiple disconnected paths")
    print("  □ Large ellipse with magenta stroke")

    print("\nREVIEW CHECKLIST:")
    print("  □ All drawing elements are visible")
    print("  □ Colors match expected values")
    print("  □ Stroke widths are correct")
    print("  □ Fill patterns are applied properly")
    print("  □ Bezier curves are smooth")
    print("  □ No rendering artifacts or distortions")

    # Basic assertions
    assert output_pdf.exists() and output_pdf.stat().st_size > 0
    assert config_json.exists() and config_json.stat().st_size > 0
    assert debug_pdf.exists() and debug_pdf.stat().st_size > 0

    print_review_footer()


@pytest.mark.human_review
@pytest.mark.e2e
@pytest.mark.skip(reason="Human review test - run manually with -m human_review")
def test_full_pipeline_human_review(human_review_output_dir, review_unique_id):
    """Complete pipeline test for human review with mixed content"""

    print_review_header("Full Pipeline Processing")

    # Create mixed content elements
    mixed_elements = [
        {"type": "text", "id": "title", "content": "Mixed Content Test Document"},
        {"type": "rectangle", "id": "header_bg", "content": "Header Background"},
        {
            "type": "text",
            "id": "body1",
            "content": "This is body text with various formatting.",
        },
        {
            "type": "text",
            "id": "body2",
            "content": "Second paragraph for layout testing.",
        },
        {"type": "rectangle", "id": "sidebar", "content": "Sidebar Element"},
        {"type": "text", "id": "footer", "content": "Footer text content"},
    ]

    # Generate unique file paths
    unique = review_unique_id
    input_pdf = human_review_output_dir / f"mixed_input_{unique}.pdf"
    output_pdf = human_review_output_dir / f"mixed_output_{unique}.pdf"
    config_json = human_review_output_dir / f"mixed_config_{unique}.json"
    debug_pdf = human_review_output_dir / f"mixed_debug_{unique}.pdf"
    comparison_report = human_review_output_dir / f"mixed_comparison_{unique}.json"

    # Step 1: Create input PDF
    create_pdf_with_elements(mixed_elements, str(input_pdf))

    # Step 2: Extract configuration
    args_extract = argparse.Namespace(
        log_level="DEBUG",
        mode="extract",
        input=str(input_pdf),
        output=None,
        config_file=str(config_json),
        debugoutput=None,
        extract_text=True,
        extract_images=True,
        extract_drawings=True,
        extract_raw_backgrounds=False,
        output_dir="./output",
        test_output_dir=str(human_review_output_dir),
        reports_output_dir="./output/reports",
        temp_dir=None,
    )

    run_pipeline_with_capture(args_extract)

    # Step 3: Generate output PDF
    args_generate = argparse.Namespace(
        log_level="DEBUG",
        mode="generate",
        input=None,
        output=str(output_pdf),
        config_file=str(config_json),
        debugoutput=None,
        extract_text=True,
        extract_images=True,
        extract_drawings=True,
        extract_raw_backgrounds=False,
        output_dir="./output",
        test_output_dir=str(human_review_output_dir),
        reports_output_dir="./output/reports",
        temp_dir=None,
    )

    run_pipeline_with_capture(args_generate)

    # Step 4: Generate debug PDF
    if config_json.exists():
        generate_debug_pdf_layers(str(config_json), str(debug_pdf))

    # Step 5: Generate detailed comparison
    comparison_result = None
    if input_pdf.exists() and output_pdf.exists():
        try:
            comparison_result = validate_documents(str(input_pdf), str(output_pdf))

            # Save comparison report
            comparison_data = {
                "input_file": str(input_pdf),
                "output_file": str(output_pdf),
                "ssim_score": comparison_result.ssim_score,
                "test_elements": len(mixed_elements),
                "timestamp": str(review_unique_id),
            }

            with open(comparison_report, "w") as f:
                json.dump(comparison_data, f, indent=2)

        except Exception as e:
            print(f"Warning: Could not generate comparison: {e}")

    # Print comprehensive review instructions
    print("FULL PIPELINE REVIEW:")
    print(f"  Input PDF:         {input_pdf}")
    print(f"  Output PDF:        {output_pdf}")
    print(f"  Config JSON:       {config_json}")
    print(f"  Debug PDF:         {debug_pdf}")
    print(f"  Comparison Report: {comparison_report}")

    if comparison_result:
        print(f"  SSIM Score:        {comparison_result.ssim_score:.4f}")
        if comparison_result.ssim_score >= 0.99:
            print("  ✅ Excellent visual fidelity")
        elif comparison_result.ssim_score >= 0.95:
            print("  ✅ Good visual fidelity")
        elif comparison_result.ssim_score >= 0.90:
            print("  ⚠️  Acceptable but check for issues")
        else:
            print("  ❌ Poor visual fidelity - investigate")

    print("\nDETAILED REVIEW CHECKLIST:")
    print("  CONTENT FIDELITY:")
    print("    □ All text content is preserved")
    print("    □ Text positioning matches original")
    print("    □ Font styles are consistent")
    print("    □ All drawing elements are present")
    print("    □ Colors and fills are accurate")

    print("  LAYOUT ACCURACY:")
    print("    □ Element spacing is maintained")
    print("    □ Page dimensions are correct")
    print("    □ No overlapping elements")
    print("    □ Proper layer ordering")

    print("  TECHNICAL QUALITY:")
    print("    □ No rendering artifacts")
    print("    □ Clean vector graphics")
    print("    □ Proper font embedding")
    print("    □ File size is reasonable")

    print("\nFILES FOR MANUAL COMPARISON:")
    print(f"  1. Open: {input_pdf}")
    print(f"  2. Open: {output_pdf}")
    print("  3. Compare side-by-side for visual differences")
    print(f"  4. Check debug PDF: {debug_pdf}")
    print(f"  5. Review config: {config_json}")

    # Basic assertions
    assert input_pdf.exists() and input_pdf.stat().st_size > 0
    assert output_pdf.exists() and output_pdf.stat().st_size > 0
    assert config_json.exists() and config_json.stat().st_size > 0

    print_review_footer()


def pytest_addoption(parser):
    """Add command line options for filtering tests"""
    parser.addoption(
        "--range",
        action="store",
        default=None,
        help="Range of elements to test (e.g., 1-5 or 3)",
    )
    parser.addoption(
        "--draw_type",
        action="store",
        default=None,
        help="Types of elements to test (e.g., text,rectangle)",
    )


@pytest.mark.human_review
@pytest.mark.skip(reason="Human review test - run manually with -m human_review")
@pytest.mark.parametrize("content_type", ["text_only", "drawings_only", "mixed"])
def test_parametrized_human_review(human_review_output_dir, review_unique_id, content_type):
    """Parametrized test for different content types"""

    print_review_header(f"Parametrized Review: {content_type}")

    # Define elements based on content type
    if content_type == "text_only":
        elements = [{"type": "text", "id": f"text_{i}", "content": f"Text Element {i}"} for i in range(1, 4)]
    elif content_type == "drawings_only":
        elements = [{"type": "rectangle", "id": f"rect_{i}", "content": f"Rectangle {i}"} for i in range(1, 4)]
    else:  # mixed
        elements = [
            {"type": "text", "id": "text_1", "content": "Mixed Content Text"},
            {"type": "rectangle", "id": "rect_1", "content": "Mixed Content Rectangle"},
        ]

    # Generate files
    unique = f"{content_type}_{review_unique_id}"
    input_pdf = human_review_output_dir / f"param_input_{unique}.pdf"
    output_pdf = human_review_output_dir / f"param_output_{unique}.pdf"
    config_json = human_review_output_dir / f"param_config_{unique}.json"

    # Run pipeline
    create_pdf_with_elements(elements, str(input_pdf))

    # Extract and generate
    for mode, output_file, config_file in [
        ("extract", None, str(config_json)),
        ("generate", str(output_pdf), str(config_json)),
    ]:
        args = argparse.Namespace(
            log_level="DEBUG",
            mode=mode,
            input=str(input_pdf) if mode == "extract" else None,
            output=output_file,
            config_file=config_file,
            debugoutput=None,
            extract_text=True,
            extract_images=True,
            extract_drawings=True,
            extract_raw_backgrounds=False,
            output_dir="./output",
            test_output_dir=str(human_review_output_dir),
            reports_output_dir="./output/reports",
            temp_dir=None,
        )
        run_pipeline_with_capture(args)

    print(f"PARAMETRIZED REVIEW ({content_type.upper()}):")
    print(f"  Input:  {input_pdf}")
    print(f"  Output: {output_pdf}")
    print(f"  Config: {config_json}")
    print(f"  Focus:  {content_type.replace('_', ' ').title()} processing")

    assert input_pdf.exists() and output_pdf.exists() and config_json.exists()

    print_review_footer()
