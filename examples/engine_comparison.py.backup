#!/usr/bin/env python3
"""
Example script demonstrating engine comparison and selection.
Shows how to use different engines and compare their performance.
"""

import os
import time
from typing import Any

from src.engine.document_parser import get_parser_for_file, parse_document
from src.settings import get_nested_config_value, set_nested_config_value


def compare_input_engines(file_path: str) -> dict[str, Any]:
    """Compare different input engines for the same file"""
    results = {}

    # Get file extension to determine available engines
    file_ext = os.path.splitext(file_path)[1].lower()

    if file_ext == ".pdf":
        engines = ["fitz"]
    elif file_ext == ".psd":
        engines = ["psd-tools", "wand"]
    else:
        print(f"Unsupported file format: {file_ext}")
        return results

    for engine in engines:
        print(f"\nTesting {engine} engine...")

        try:
            # Set the engine
            set_nested_config_value("engines.input.default", engine)

            # Measure parsing time
            start_time = time.time()
            document = parse_document(file_path)
            end_time = time.time()

            results[engine] = {
                "success": True,
                "parse_time": end_time - start_time,
                "document_version": document.version,
                "engine_version": document.engine_version,
                "num_pages": len(document.document_structure),
                "total_elements": sum(
                    len(layer.content)
                    for page in document.document_structure
                    for layer in page.layers
                ),
            }

            print(f"  ✓ Parsed in {results[engine]['parse_time']:.2f}s")
            print(f"  ✓ Found {results[engine]['total_elements']} elements")

        except Exception as e:
            results[engine] = {"success": False, "error": str(e), "parse_time": 0}
            print(f"  ✗ Failed: {e}")

    return results


def compare_output_engines(document, output_base_path: str) -> dict[str, Any]:
    """Compare different output engines for the same document"""
    results = {}
    engines = ["reportlab", "pymupdf"]

    for engine in engines:
        print(f"\nTesting {engine} output engine...")

        try:
            # Set the engine
            set_nested_config_value("engines.output.default", engine)

            output_path = f"{output_base_path}_{engine}.pdf"

            # Measure rendering time
            start_time = time.time()

            # This would use the actual rendering engine
            # For now, we'll simulate the process
            print(f"  Would render to: {output_path}")

            end_time = time.time()

            results[engine] = {
                "success": True,
                "render_time": end_time - start_time,
                "output_path": output_path,
                "file_size": 0,  # Would measure actual file size
            }

            print(f"  ✓ Rendered in {results[engine]['render_time']:.2f}s")

        except Exception as e:
            results[engine] = {"success": False, "error": str(e), "render_time": 0}
            print(f"  ✗ Failed: {e}")

    return results


def demonstrate_engine_selection():
    """Demonstrate different ways to select engines"""

    print("=== Engine Selection Demonstration ===\n")

    # 1. Automatic engine selection
    print("1. Automatic Engine Selection:")
    test_files = ["test.pdf", "test.psd", "test.docx"]

    for file_path in test_files:
        parser = get_parser_for_file(file_path)
        if parser:
            print(f"  {file_path} -> {parser.__class__.__name__}")
        else:
            print(f"  {file_path} -> No suitable parser found")

    # 2. Configuration-based selection
    print("\n2. Configuration-based Engine Selection:")

    # Show current defaults
    input_default = get_nested_config_value("engines.input.default")
    output_default = get_nested_config_value("engines.output.default")
    print(f"  Current input default: {input_default}")
    print(f"  Current output default: {output_default}")

    # Change defaults
    set_nested_config_value("engines.input.default", "wand")
    set_nested_config_value("engines.output.default", "pymupdf")

    new_input_default = get_nested_config_value("engines.input.default")
    new_output_default = get_nested_config_value("engines.output.default")
    print(f"  New input default: {new_input_default}")
    print(f"  New output default: {new_output_default}")

    # 3. Engine-specific configuration
    print("\n3. Engine-specific Configuration:")

    wand_config = get_nested_config_value("engines.input.wand")
    print(f"  Wand configuration: {wand_config}")

    reportlab_config = get_nested_config_value("engines.output.reportlab")
    print(f"  ReportLab configuration: {reportlab_config}")


def performance_comparison(file_path: str):
    """Perform a comprehensive performance comparison"""

    print(f"\n=== Performance Comparison for {file_path} ===\n")

    if not os.path.exists(file_path):
        print(f"Test file not found: {file_path}")
        print("Please provide a valid test file for comparison")
        return

    # Compare input engines
    input_results = compare_input_engines(file_path)

    if not input_results:
        print("No input engines could process the file")
        return

    # Find the fastest input engine
    successful_engines = {k: v for k, v in input_results.items() if v["success"]}
    if successful_engines:
        fastest_input = min(
            successful_engines.keys(), key=lambda k: successful_engines[k]["parse_time"]
        )
        print(
            f"\nFastest input engine: {fastest_input} ({successful_engines[fastest_input]['parse_time']:.2f}s)"
        )

    # Compare output engines (simulated)
    output_base = os.path.splitext(file_path)[0] + "_output"
    output_results = compare_output_engines(None, output_base)

    # Print summary
    print("\n=== Summary ===")
    print("\nInput Engine Results:")
    for engine, result in input_results.items():
        if result["success"]:
            print(
                f"  {engine}: {result['parse_time']:.2f}s, {result['total_elements']} elements"
            )
        else:
            print(f"  {engine}: FAILED - {result['error']}")

    print("\nOutput Engine Results:")
    for engine, result in output_results.items():
        if result["success"]:
            print(f"  {engine}: {result['render_time']:.2f}s")
        else:
            print(f"  {engine}: FAILED - {result['error']}")


def main():
    """Main demonstration function"""

    print("Multi-Format Document Engine - Engine Comparison Example")
    print("=" * 60)

    # Demonstrate engine selection
    demonstrate_engine_selection()

    # Performance comparison with test files
    test_files = [
        "tests/fixtures/sample.pdf",
        "tests/fixtures/sample.psd",
        "input/sample.pdf",  # Fallback
    ]

    for test_file in test_files:
        if os.path.exists(test_file):
            performance_comparison(test_file)
            break
    else:
        print("\nNo test files found for performance comparison")
        print("Available test files should be placed in:")
        for test_file in test_files:
            print(f"  - {test_file}")

    print("\n" + "=" * 60)
    print("Engine comparison complete!")
    print("\nTo run with specific engines:")
    print("  python main.py --input-engine wand --output-engine pymupdf")
    print("  python main.py --input-engine fitz --output-engine reportlab")


if __name__ == "__main__":
    main()
